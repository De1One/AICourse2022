{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "RnO7TOjCxDat",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Force CPU\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0 = all messages are logged, 3 - INFO, WARNING, and ERROR messages are not printed\n",
    "# Загружаем все необходимые библиотеки\n",
    "import string \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector, Dropout,TimeDistributed,Bidirectional,GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import load_model \n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Задаем функцию для чтения файла, а точнее текста, который представляет из себя две колонки, разделенные табуляцией.\n",
    "def read_text(filename):\n",
    "    with open(filename, mode='rt', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        sents = text.strip().split('\\n')\n",
    "        return [i.split('\\t') for i in sents]"
   ],
   "metadata": {
    "id": "TD_S3l4nx16v",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data = read_text(\"deutch.txt\")\n",
    "deu_eng = np.array(data)\n",
    "\n",
    "deu_eng = deu_eng[:60000,:]\n",
    "print(\"Dictionary size:\", deu_eng.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H1KnYFYax387",
    "outputId": "80aa58a4-bc8b-4820-d752-5cd19e4515ca",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 41,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dictionary size: (60000, 2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]] \n",
    "deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]] "
   ],
   "metadata": {
    "id": "VGx2uKPFx_RU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(len(deu_eng)): \n",
    "    deu_eng[i,0] = deu_eng[i,0].lower() \n",
    "    deu_eng[i,1] = deu_eng[i,1].lower()"
   ],
   "metadata": {
    "id": "1M54bOoeyBqL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(deu_eng[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1 \n",
    "eng_length = 8"
   ],
   "metadata": {
    "id": "gUVs8P6gyD9H",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "deu_tokenizer = Tokenizer()\n",
    "deu_tokenizer.fit_on_texts(deu_eng[:, 1])\n",
    "deu_vocab_size = len(deu_tokenizer.word_index) + 1 \n",
    "deu_length = 8 "
   ],
   "metadata": {
    "id": "R9OSigVjyGMq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def encode_sequences(tokenizer, length, lines):          \n",
    "    # integer encode sequences\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
    "    return seq"
   ],
   "metadata": {
    "id": "mZdzdrXLyJX_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train, test = train_test_split(deu_eng, test_size=0.2, random_state=12)"
   ],
   "metadata": {
    "id": "jEGZMJuxyMaD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_sequences(deu_tokenizer, deu_length, train[:, 1])\n",
    "testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_sequences(deu_tokenizer, deu_length, test[:, 1])"
   ],
   "metadata": {
    "id": "Rsm4VaZbyOU0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def make_model(in_vocab, out_vocab, in_timesteps, out_timesteps, n):\n",
    "    #model = Sequential()\n",
    "    #model.add(Embedding(in_vocab, n, input_length=in_timesteps, mask_zero=True))\n",
    "    #model.add(LSTM(n))\n",
    "    #model.add(Dropout(0.3))\n",
    "    #model.add(RepeatVector(out_timesteps))\n",
    "    #model.add(LSTM(n, return_sequences=True))\n",
    "    #model.add(Dropout(0.3))\n",
    "    #model.add(TimeDistributed(Dense(out_vocab, activation='softmax')))\n",
    "    #model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy')\n",
    "    learning_rate = 0.002\n",
    "\n",
    "    # Build the layers    \n",
    "    model = Sequential()\n",
    "    # Embedding\n",
    "    model.add(Embedding(in_vocab, n, input_length=in_timesteps))\n",
    "    # Encoder\n",
    "    model.add(LSTM(n))\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    model.add(Bidirectional(GRU(n)))\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    # Decoder\n",
    "    model.add(Bidirectional(GRU(n, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(n, activation='relu')))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(TimeDistributed(Dense(out_vocab, activation='softmax')))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=optimizers.RMSprop(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(\"deu_vocab_size:\", deu_vocab_size, deu_length)\n",
    "print(\"eng_vocab_size:\", eng_vocab_size, eng_length)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Etpwzr3CyRxc",
    "outputId": "6d7eefef-81ac-4672-f90d-27c4b5636f68",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 49,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "deu_vocab_size: 12159 8\n",
      "eng_vocab_size: 7068 8\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = make_model(eng_vocab_size, deu_vocab_size, eng_length, deu_length, 512)"
   ],
   "metadata": {
    "id": "Q30EIJyQyTXn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "num_epochs = 250 #250\n",
    "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), epochs=num_epochs, batch_size=512, validation_split=0.2, callbacks=None, verbose=1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()\n",
    "model.save('en-de-model.h5')"
   ],
   "metadata": {
    "id": "33jZbNamyV0t",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "0c2a43fe-a0c0-4343-a61d-e1c6d9277ca1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/250\n",
      "25/25 [==============================] - 13s 154ms/step - loss: 4.1244 - val_loss: 3.0209\n",
      "Epoch 2/250\n",
      "25/25 [==============================] - 2s 71ms/step - loss: 2.9707 - val_loss: 2.8405\n",
      "Epoch 3/250\n",
      "25/25 [==============================] - 2s 76ms/step - loss: 2.7886 - val_loss: 2.8210\n",
      "Epoch 4/250\n",
      "25/25 [==============================] - 2s 70ms/step - loss: 2.7088 - val_loss: 2.7938\n",
      "Epoch 5/250\n",
      "25/25 [==============================] - 2s 76ms/step - loss: 2.6259 - val_loss: 2.6701\n",
      "Epoch 6/250\n",
      "25/25 [==============================] - 2s 70ms/step - loss: 2.5120 - val_loss: 2.6025\n",
      "Epoch 7/250\n",
      "25/25 [==============================] - 2s 69ms/step - loss: 2.4379 - val_loss: 2.5248\n",
      "Epoch 8/250\n",
      "25/25 [==============================] - 2s 79ms/step - loss: 2.3592 - val_loss: 2.4620\n",
      "Epoch 9/250\n",
      "25/25 [==============================] - 2s 71ms/step - loss: 2.2854 - val_loss: 2.4168\n",
      "Epoch 10/250\n",
      "25/25 [==============================] - 2s 70ms/step - loss: 2.2236 - val_loss: 2.3764\n",
      "Epoch 11/250\n",
      "25/25 [==============================] - 2s 70ms/step - loss: 2.1649 - val_loss: 2.3379\n",
      "Epoch 12/250\n",
      "25/25 [==============================] - 2s 70ms/step - loss: 2.1095 - val_loss: 2.3458\n",
      "Epoch 13/250\n",
      "25/25 [==============================] - 2s 77ms/step - loss: 2.0591 - val_loss: 2.2718\n",
      "Epoch 14/250\n",
      "25/25 [==============================] - 2s 72ms/step - loss: 2.0156 - val_loss: 2.2448\n",
      "Epoch 15/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 1.9626 - val_loss: 2.2287\n",
      "Epoch 16/250\n",
      "25/25 [==============================] - 2s 78ms/step - loss: 1.9195 - val_loss: 2.1848\n",
      "Epoch 17/250\n",
      "25/25 [==============================] - 2s 71ms/step - loss: 1.8742 - val_loss: 2.1621\n",
      "Epoch 18/250\n",
      "25/25 [==============================] - 2s 71ms/step - loss: 1.8328 - val_loss: 2.1821\n",
      "Epoch 19/250\n",
      "25/25 [==============================] - 2s 72ms/step - loss: 1.7949 - val_loss: 2.1281\n",
      "Epoch 20/250\n",
      "25/25 [==============================] - 2s 71ms/step - loss: 1.7540 - val_loss: 2.1034\n",
      "Epoch 21/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 1.7085 - val_loss: 2.0851\n",
      "Epoch 22/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 1.6735 - val_loss: 2.0559\n",
      "Epoch 23/250\n",
      "25/25 [==============================] - 2s 72ms/step - loss: 1.6337 - val_loss: 2.0424\n",
      "Epoch 24/250\n",
      "25/25 [==============================] - 2s 72ms/step - loss: 1.5992 - val_loss: 2.0372\n",
      "Epoch 25/250\n",
      "25/25 [==============================] - 2s 78ms/step - loss: 1.5622 - val_loss: 2.0106\n",
      "Epoch 26/250\n",
      "25/25 [==============================] - 2s 72ms/step - loss: 1.5254 - val_loss: 1.9727\n",
      "Epoch 27/250\n",
      "25/25 [==============================] - 2s 79ms/step - loss: 1.4916 - val_loss: 1.9628\n",
      "Epoch 28/250\n",
      "25/25 [==============================] - 2s 76ms/step - loss: 1.4548 - val_loss: 1.9390\n",
      "Epoch 29/250\n",
      "25/25 [==============================] - 2s 79ms/step - loss: 1.4243 - val_loss: 1.9303\n",
      "Epoch 30/250\n",
      "25/25 [==============================] - 2s 79ms/step - loss: 1.3916 - val_loss: 1.9268\n",
      "Epoch 31/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 1.3560 - val_loss: 1.8980\n",
      "Epoch 32/250\n",
      "25/25 [==============================] - 2s 73ms/step - loss: 1.3322 - val_loss: 1.8970\n",
      "Epoch 33/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 1.2976 - val_loss: 1.8770\n",
      "Epoch 34/250\n",
      "25/25 [==============================] - 2s 76ms/step - loss: 1.2693 - val_loss: 1.8807\n",
      "Epoch 35/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 1.2405 - val_loss: 1.8462\n",
      "Epoch 36/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 1.2136 - val_loss: 1.8347\n",
      "Epoch 37/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 1.1789 - val_loss: 1.8205\n",
      "Epoch 38/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 1.1602 - val_loss: 1.8277\n",
      "Epoch 39/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 1.1282 - val_loss: 1.8057\n",
      "Epoch 40/250\n",
      "25/25 [==============================] - 2s 82ms/step - loss: 1.1043 - val_loss: 1.7988\n",
      "Epoch 41/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 1.0791 - val_loss: 1.7883\n",
      "Epoch 42/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 1.0562 - val_loss: 1.7797\n",
      "Epoch 43/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 1.0295 - val_loss: 1.7690\n",
      "Epoch 44/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 1.0089 - val_loss: 1.7662\n",
      "Epoch 45/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.9843 - val_loss: 1.7519\n",
      "Epoch 46/250\n",
      "25/25 [==============================] - 2s 82ms/step - loss: 0.9588 - val_loss: 1.7624\n",
      "Epoch 47/250\n",
      "25/25 [==============================] - 2s 76ms/step - loss: 0.9401 - val_loss: 1.7467\n",
      "Epoch 48/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.9191 - val_loss: 1.7374\n",
      "Epoch 49/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.8955 - val_loss: 1.7312\n",
      "Epoch 50/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.8777 - val_loss: 1.7263\n",
      "Epoch 51/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.8539 - val_loss: 1.7255\n",
      "Epoch 52/250\n",
      "25/25 [==============================] - 2s 76ms/step - loss: 0.8391 - val_loss: 1.7089\n",
      "Epoch 53/250\n",
      "25/25 [==============================] - 2s 78ms/step - loss: 0.8205 - val_loss: 1.7085\n",
      "Epoch 54/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.7996 - val_loss: 1.7054\n",
      "Epoch 55/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.7781 - val_loss: 1.7005\n",
      "Epoch 56/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.7641 - val_loss: 1.6957\n",
      "Epoch 57/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.7463 - val_loss: 1.6885\n",
      "Epoch 58/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.7287 - val_loss: 1.6877\n",
      "Epoch 59/250\n",
      "25/25 [==============================] - 2s 82ms/step - loss: 0.7080 - val_loss: 1.6827\n",
      "Epoch 60/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.6986 - val_loss: 1.6864\n",
      "Epoch 61/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.6801 - val_loss: 1.6806\n",
      "Epoch 62/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.6669 - val_loss: 1.6744\n",
      "Epoch 63/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.6531 - val_loss: 1.6734\n",
      "Epoch 64/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.6365 - val_loss: 1.6704\n",
      "Epoch 65/250\n",
      "25/25 [==============================] - 2s 83ms/step - loss: 0.6234 - val_loss: 1.6687\n",
      "Epoch 66/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.6105 - val_loss: 1.6742\n",
      "Epoch 67/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.5987 - val_loss: 1.6690\n",
      "Epoch 68/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.5820 - val_loss: 1.6672\n",
      "Epoch 69/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.5723 - val_loss: 1.6648\n",
      "Epoch 70/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.5601 - val_loss: 1.6657\n",
      "Epoch 71/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.5508 - val_loss: 1.6790\n",
      "Epoch 72/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.5377 - val_loss: 1.6661\n",
      "Epoch 73/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.5266 - val_loss: 1.6648\n",
      "Epoch 74/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.5162 - val_loss: 1.6596\n",
      "Epoch 75/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.5043 - val_loss: 1.6627\n",
      "Epoch 76/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.4955 - val_loss: 1.6659\n",
      "Epoch 77/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.4845 - val_loss: 1.6599\n",
      "Epoch 78/250\n",
      "25/25 [==============================] - 2s 77ms/step - loss: 0.4764 - val_loss: 1.6595\n",
      "Epoch 79/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.4680 - val_loss: 1.6641\n",
      "Epoch 80/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.4603 - val_loss: 1.6642\n",
      "Epoch 81/250\n",
      "25/25 [==============================] - 2s 73ms/step - loss: 0.4482 - val_loss: 1.6653\n",
      "Epoch 82/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.4447 - val_loss: 1.6743\n",
      "Epoch 83/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.4312 - val_loss: 1.6655\n",
      "Epoch 84/250\n",
      "25/25 [==============================] - 2s 82ms/step - loss: 0.4262 - val_loss: 1.6726\n",
      "Epoch 85/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.4178 - val_loss: 1.6711\n",
      "Epoch 86/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.4101 - val_loss: 1.6763\n",
      "Epoch 87/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.4029 - val_loss: 1.6758\n",
      "Epoch 88/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.3956 - val_loss: 1.6731\n",
      "Epoch 89/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.3870 - val_loss: 1.6759\n",
      "Epoch 90/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.3865 - val_loss: 1.6834\n",
      "Epoch 91/250\n",
      "25/25 [==============================] - 2s 76ms/step - loss: 0.3779 - val_loss: 1.6823\n",
      "Epoch 92/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.3688 - val_loss: 1.6903\n",
      "Epoch 93/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.3682 - val_loss: 1.6906\n",
      "Epoch 94/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.3589 - val_loss: 1.6900\n",
      "Epoch 95/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.3553 - val_loss: 1.6929\n",
      "Epoch 96/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.3477 - val_loss: 1.7089\n",
      "Epoch 97/250\n",
      "25/25 [==============================] - 2s 83ms/step - loss: 0.3444 - val_loss: 1.6950\n",
      "Epoch 98/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.3371 - val_loss: 1.7008\n",
      "Epoch 99/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.3337 - val_loss: 1.6981\n",
      "Epoch 100/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.3301 - val_loss: 1.7085\n",
      "Epoch 101/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.3232 - val_loss: 1.7126\n",
      "Epoch 102/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.3185 - val_loss: 1.7191\n",
      "Epoch 103/250\n",
      "25/25 [==============================] - 2s 77ms/step - loss: 0.3142 - val_loss: 1.7122\n",
      "Epoch 104/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.3106 - val_loss: 1.7208\n",
      "Epoch 105/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.3074 - val_loss: 1.7178\n",
      "Epoch 106/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.3028 - val_loss: 1.7258\n",
      "Epoch 107/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.3002 - val_loss: 1.7249\n",
      "Epoch 108/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.2974 - val_loss: 1.7362\n",
      "Epoch 109/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.2923 - val_loss: 1.7381\n",
      "Epoch 110/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.2888 - val_loss: 1.7311\n",
      "Epoch 111/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2859 - val_loss: 1.7357\n",
      "Epoch 112/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2837 - val_loss: 1.7493\n",
      "Epoch 113/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.2784 - val_loss: 1.7378\n",
      "Epoch 114/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2775 - val_loss: 1.7479\n",
      "Epoch 115/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.2725 - val_loss: 1.7521\n",
      "Epoch 116/250\n",
      "25/25 [==============================] - 2s 83ms/step - loss: 0.2720 - val_loss: 1.7595\n",
      "Epoch 117/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.2675 - val_loss: 1.7548\n",
      "Epoch 118/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2645 - val_loss: 1.7579\n",
      "Epoch 119/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2632 - val_loss: 1.7564\n",
      "Epoch 120/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2579 - val_loss: 1.7666\n",
      "Epoch 121/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.2584 - val_loss: 1.7707\n",
      "Epoch 122/250\n",
      "25/25 [==============================] - 2s 82ms/step - loss: 0.2570 - val_loss: 1.7627\n",
      "Epoch 123/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2520 - val_loss: 1.7705\n",
      "Epoch 124/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.2510 - val_loss: 1.7821\n",
      "Epoch 125/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2490 - val_loss: 1.7678\n",
      "Epoch 126/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2485 - val_loss: 1.7774\n",
      "Epoch 127/250\n",
      "25/25 [==============================] - 2s 73ms/step - loss: 0.2437 - val_loss: 1.7874\n",
      "Epoch 128/250\n",
      "25/25 [==============================] - 2s 82ms/step - loss: 0.2432 - val_loss: 1.7810\n",
      "Epoch 129/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.2411 - val_loss: 1.7875\n",
      "Epoch 130/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2417 - val_loss: 1.7897\n",
      "Epoch 131/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.2368 - val_loss: 1.7930\n",
      "Epoch 132/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2349 - val_loss: 1.7897\n",
      "Epoch 133/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.2339 - val_loss: 1.7943\n",
      "Epoch 134/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.2324 - val_loss: 1.7958\n",
      "Epoch 135/250\n",
      "25/25 [==============================] - 2s 77ms/step - loss: 0.2303 - val_loss: 1.8084\n",
      "Epoch 136/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2289 - val_loss: 1.8075\n",
      "Epoch 137/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2282 - val_loss: 1.8047\n",
      "Epoch 138/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2257 - val_loss: 1.8076\n",
      "Epoch 139/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2241 - val_loss: 1.8076\n",
      "Epoch 140/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2242 - val_loss: 1.8121\n",
      "Epoch 141/250\n",
      "25/25 [==============================] - 2s 82ms/step - loss: 0.2205 - val_loss: 1.8190\n",
      "Epoch 142/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.2199 - val_loss: 1.8113\n",
      "Epoch 143/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2196 - val_loss: 1.8249\n",
      "Epoch 144/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.2176 - val_loss: 1.8223\n",
      "Epoch 145/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.2150 - val_loss: 1.8272\n",
      "Epoch 146/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2148 - val_loss: 1.8302\n",
      "Epoch 147/250\n",
      "25/25 [==============================] - 2s 76ms/step - loss: 0.2147 - val_loss: 1.8328\n",
      "Epoch 148/250\n",
      "25/25 [==============================] - 2s 82ms/step - loss: 0.2133 - val_loss: 1.8339\n",
      "Epoch 149/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.2113 - val_loss: 1.8357\n",
      "Epoch 150/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.2124 - val_loss: 1.8362\n",
      "Epoch 151/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.2106 - val_loss: 1.8357\n",
      "Epoch 152/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.2105 - val_loss: 1.8340\n",
      "Epoch 153/250\n",
      "25/25 [==============================] - 2s 83ms/step - loss: 0.2067 - val_loss: 1.8474\n",
      "Epoch 154/250\n",
      "25/25 [==============================] - 2s 82ms/step - loss: 0.2075 - val_loss: 1.8414\n",
      "Epoch 155/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2052 - val_loss: 1.8451\n",
      "Epoch 156/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2056 - val_loss: 1.8490\n",
      "Epoch 157/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.2043 - val_loss: 1.8574\n",
      "Epoch 158/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.2048 - val_loss: 1.8498\n",
      "Epoch 159/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.2004 - val_loss: 1.8527\n",
      "Epoch 160/250\n",
      "25/25 [==============================] - 2s 76ms/step - loss: 0.2020 - val_loss: 1.8563\n",
      "Epoch 161/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.2003 - val_loss: 1.8569\n",
      "Epoch 162/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.2002 - val_loss: 1.8578\n",
      "Epoch 163/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1974 - val_loss: 1.8639\n",
      "Epoch 164/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1995 - val_loss: 1.8653\n",
      "Epoch 165/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.1940 - val_loss: 1.8624\n",
      "Epoch 166/250\n",
      "25/25 [==============================] - 2s 77ms/step - loss: 0.1956 - val_loss: 1.8678\n",
      "Epoch 167/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1960 - val_loss: 1.8755\n",
      "Epoch 168/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1938 - val_loss: 1.8741\n",
      "Epoch 169/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1950 - val_loss: 1.8733\n",
      "Epoch 170/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1946 - val_loss: 1.8786\n",
      "Epoch 171/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1921 - val_loss: 1.8747\n",
      "Epoch 172/250\n",
      "25/25 [==============================] - 2s 83ms/step - loss: 0.1911 - val_loss: 1.8785\n",
      "Epoch 173/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.1891 - val_loss: 1.8853\n",
      "Epoch 174/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1898 - val_loss: 1.8790\n",
      "Epoch 175/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1904 - val_loss: 1.8808\n",
      "Epoch 176/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1908 - val_loss: 1.8884\n",
      "Epoch 177/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.1890 - val_loss: 1.8861\n",
      "Epoch 178/250\n",
      "25/25 [==============================] - 2s 82ms/step - loss: 0.1853 - val_loss: 1.8903\n",
      "Epoch 179/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1897 - val_loss: 1.8904\n",
      "Epoch 180/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1843 - val_loss: 1.8946\n",
      "Epoch 181/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1877 - val_loss: 1.8880\n",
      "Epoch 182/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.1863 - val_loss: 1.8964\n",
      "Epoch 183/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1868 - val_loss: 1.8980\n",
      "Epoch 184/250\n",
      "25/25 [==============================] - 2s 82ms/step - loss: 0.1854 - val_loss: 1.8968\n",
      "Epoch 185/250\n",
      "25/25 [==============================] - 2s 82ms/step - loss: 0.1823 - val_loss: 1.9001\n",
      "Epoch 186/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.1837 - val_loss: 1.9005\n",
      "Epoch 187/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1842 - val_loss: 1.8991\n",
      "Epoch 188/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1827 - val_loss: 1.9016\n",
      "Epoch 189/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1825 - val_loss: 1.8990\n",
      "Epoch 190/250\n",
      "25/25 [==============================] - 2s 82ms/step - loss: 0.1825 - val_loss: 1.9078\n",
      "Epoch 191/250\n",
      "25/25 [==============================] - 2s 76ms/step - loss: 0.1799 - val_loss: 1.9071\n",
      "Epoch 192/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1806 - val_loss: 1.9064\n",
      "Epoch 193/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1802 - val_loss: 1.9155\n",
      "Epoch 194/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1823 - val_loss: 1.9158\n",
      "Epoch 195/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1804 - val_loss: 1.9107\n",
      "Epoch 196/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.1789 - val_loss: 1.9253\n",
      "Epoch 197/250\n",
      "25/25 [==============================] - 2s 84ms/step - loss: 0.1766 - val_loss: 1.9165\n",
      "Epoch 198/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.1790 - val_loss: 1.9251\n",
      "Epoch 199/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1796 - val_loss: 1.9228\n",
      "Epoch 200/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.1786 - val_loss: 1.9290\n",
      "Epoch 201/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1773 - val_loss: 1.9255\n",
      "Epoch 202/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.1762 - val_loss: 1.9249\n",
      "Epoch 203/250\n",
      "25/25 [==============================] - 2s 83ms/step - loss: 0.1776 - val_loss: 1.9293\n",
      "Epoch 204/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1756 - val_loss: 1.9298\n",
      "Epoch 205/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1754 - val_loss: 1.9290\n",
      "Epoch 206/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1732 - val_loss: 1.9284\n",
      "Epoch 207/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1766 - val_loss: 1.9344\n",
      "Epoch 208/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1750 - val_loss: 1.9346\n",
      "Epoch 209/250\n",
      "25/25 [==============================] - 2s 77ms/step - loss: 0.1743 - val_loss: 1.9332\n",
      "Epoch 210/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1739 - val_loss: 1.9392\n",
      "Epoch 211/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1732 - val_loss: 1.9332\n",
      "Epoch 212/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1732 - val_loss: 1.9343\n",
      "Epoch 213/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1727 - val_loss: 1.9411\n",
      "Epoch 214/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1705 - val_loss: 1.9448\n",
      "Epoch 215/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.1727 - val_loss: 1.9450\n",
      "Epoch 216/250\n",
      "25/25 [==============================] - 2s 82ms/step - loss: 0.1735 - val_loss: 1.9451\n",
      "Epoch 217/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1715 - val_loss: 1.9482\n",
      "Epoch 218/250\n",
      "25/25 [==============================] - 2s 80ms/step - loss: 0.1734 - val_loss: 1.9518\n",
      "Epoch 219/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.1697 - val_loss: 1.9539\n",
      "Epoch 220/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1718 - val_loss: 1.9529\n",
      "Epoch 221/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1716 - val_loss: 1.9578\n",
      "Epoch 222/250\n",
      "25/25 [==============================] - 2s 76ms/step - loss: 0.1682 - val_loss: 1.9534\n",
      "Epoch 223/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.1685 - val_loss: 1.9549\n",
      "Epoch 224/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1707 - val_loss: 1.9550\n",
      "Epoch 225/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.1682 - val_loss: 1.9560\n",
      "Epoch 226/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1674 - val_loss: 1.9603\n",
      "Epoch 227/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1665 - val_loss: 1.9638\n",
      "Epoch 228/250\n",
      "25/25 [==============================] - 2s 82ms/step - loss: 0.1689 - val_loss: 1.9671\n",
      "Epoch 229/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1687 - val_loss: 1.9590\n",
      "Epoch 230/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1684 - val_loss: 1.9645\n",
      "Epoch 231/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.1665 - val_loss: 1.9638\n",
      "Epoch 232/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.1679 - val_loss: 1.9681\n",
      "Epoch 233/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1682 - val_loss: 1.9668\n",
      "Epoch 234/250\n",
      "25/25 [==============================] - 2s 82ms/step - loss: 0.1668 - val_loss: 1.9696\n",
      "Epoch 235/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.1676 - val_loss: 1.9634\n",
      "Epoch 236/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1669 - val_loss: 1.9686\n",
      "Epoch 237/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1644 - val_loss: 1.9669\n",
      "Epoch 238/250\n",
      "25/25 [==============================] - 2s 74ms/step - loss: 0.1674 - val_loss: 1.9666\n",
      "Epoch 239/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.1656 - val_loss: 1.9675\n",
      "Epoch 240/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.1663 - val_loss: 1.9681\n",
      "Epoch 241/250\n",
      "25/25 [==============================] - 2s 82ms/step - loss: 0.1658 - val_loss: 1.9786\n",
      "Epoch 242/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1663 - val_loss: 1.9746\n",
      "Epoch 243/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.1652 - val_loss: 1.9768\n",
      "Epoch 244/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.1644 - val_loss: 1.9706\n",
      "Epoch 245/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.1635 - val_loss: 1.9783\n",
      "Epoch 246/250\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.1657 - val_loss: 1.9809\n",
      "Epoch 247/250\n",
      "25/25 [==============================] - 2s 83ms/step - loss: 0.1633 - val_loss: 1.9808\n",
      "Epoch 248/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1646 - val_loss: 1.9819\n",
      "Epoch 249/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1636 - val_loss: 1.9827\n",
      "Epoch 250/250\n",
      "25/25 [==============================] - 2s 81ms/step - loss: 0.1651 - val_loss: 1.9827\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "num_epochs = 100 #250\n",
    "model = make_model(eng_vocab_size, deu_vocab_size, eng_length, deu_length, 128)\n",
    "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), epochs=num_epochs, batch_size=128, validation_split=0.2, callbacks=None, verbose=2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()\n",
    "model.save('en-de-model.h5')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3tTctIdAYKh",
    "outputId": "16ff13ad-c001-4207-ef70-d6a247de2163",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "300/300 - 19s - loss: 3.7547 - accuracy: 0.4980 - val_loss: 3.3085 - val_accuracy: 0.5309 - 19s/epoch - 62ms/step\n",
      "Epoch 2/100\n",
      "300/300 - 10s - loss: 3.2463 - accuracy: 0.5382 - val_loss: 3.0588 - val_accuracy: 0.5481 - 10s/epoch - 35ms/step\n",
      "Epoch 3/100\n",
      "300/300 - 11s - loss: 3.0280 - accuracy: 0.5633 - val_loss: 2.8952 - val_accuracy: 0.5754 - 11s/epoch - 38ms/step\n",
      "Epoch 4/100\n",
      "300/300 - 10s - loss: 2.9017 - accuracy: 0.5760 - val_loss: 2.8456 - val_accuracy: 0.5854 - 10s/epoch - 34ms/step\n",
      "Epoch 5/100\n",
      "300/300 - 10s - loss: 2.8137 - accuracy: 0.5864 - val_loss: 2.7509 - val_accuracy: 0.5953 - 10s/epoch - 34ms/step\n",
      "Epoch 6/100\n",
      "300/300 - 11s - loss: 2.7341 - accuracy: 0.5940 - val_loss: 2.6857 - val_accuracy: 0.6013 - 11s/epoch - 38ms/step\n",
      "Epoch 7/100\n",
      "300/300 - 10s - loss: 2.6657 - accuracy: 0.6009 - val_loss: 2.6429 - val_accuracy: 0.6047 - 10s/epoch - 34ms/step\n",
      "Epoch 8/100\n",
      "300/300 - 10s - loss: 2.6098 - accuracy: 0.6066 - val_loss: 2.5799 - val_accuracy: 0.6150 - 10s/epoch - 34ms/step\n",
      "Epoch 9/100\n",
      "300/300 - 11s - loss: 2.5598 - accuracy: 0.6124 - val_loss: 2.5389 - val_accuracy: 0.6211 - 11s/epoch - 38ms/step\n",
      "Epoch 10/100\n",
      "300/300 - 10s - loss: 2.5110 - accuracy: 0.6167 - val_loss: 2.5095 - val_accuracy: 0.6258 - 10s/epoch - 34ms/step\n",
      "Epoch 11/100\n",
      "300/300 - 11s - loss: 2.4677 - accuracy: 0.6220 - val_loss: 2.4822 - val_accuracy: 0.6309 - 11s/epoch - 38ms/step\n",
      "Epoch 12/100\n",
      "300/300 - 11s - loss: 2.4290 - accuracy: 0.6250 - val_loss: 2.4417 - val_accuracy: 0.6335 - 11s/epoch - 38ms/step\n",
      "Epoch 13/100\n",
      "300/300 - 10s - loss: 2.3974 - accuracy: 0.6290 - val_loss: 2.4307 - val_accuracy: 0.6356 - 10s/epoch - 34ms/step\n",
      "Epoch 14/100\n",
      "300/300 - 11s - loss: 2.3651 - accuracy: 0.6322 - val_loss: 2.4105 - val_accuracy: 0.6400 - 11s/epoch - 38ms/step\n",
      "Epoch 15/100\n",
      "300/300 - 10s - loss: 2.3306 - accuracy: 0.6359 - val_loss: 2.3862 - val_accuracy: 0.6407 - 10s/epoch - 34ms/step\n",
      "Epoch 16/100\n",
      "300/300 - 10s - loss: 2.3021 - accuracy: 0.6385 - val_loss: 2.3490 - val_accuracy: 0.6447 - 10s/epoch - 34ms/step\n",
      "Epoch 17/100\n",
      "300/300 - 11s - loss: 2.2767 - accuracy: 0.6416 - val_loss: 2.3716 - val_accuracy: 0.6482 - 11s/epoch - 38ms/step\n",
      "Epoch 18/100\n",
      "300/300 - 10s - loss: 2.2547 - accuracy: 0.6441 - val_loss: 2.3475 - val_accuracy: 0.6479 - 10s/epoch - 34ms/step\n",
      "Epoch 19/100\n",
      "300/300 - 10s - loss: 2.2315 - accuracy: 0.6476 - val_loss: 2.3262 - val_accuracy: 0.6510 - 10s/epoch - 34ms/step\n",
      "Epoch 20/100\n",
      "300/300 - 11s - loss: 2.2051 - accuracy: 0.6495 - val_loss: 2.2966 - val_accuracy: 0.6554 - 11s/epoch - 38ms/step\n",
      "Epoch 21/100\n",
      "300/300 - 11s - loss: 2.1786 - accuracy: 0.6518 - val_loss: 2.3221 - val_accuracy: 0.6550 - 11s/epoch - 38ms/step\n",
      "Epoch 22/100\n",
      "300/300 - 11s - loss: 2.1595 - accuracy: 0.6543 - val_loss: 2.3381 - val_accuracy: 0.6583 - 11s/epoch - 38ms/step\n",
      "Epoch 23/100\n",
      "300/300 - 10s - loss: 2.1383 - accuracy: 0.6576 - val_loss: 2.3303 - val_accuracy: 0.6606 - 10s/epoch - 34ms/step\n",
      "Epoch 24/100\n",
      "300/300 - 11s - loss: 2.1240 - accuracy: 0.6599 - val_loss: 2.2700 - val_accuracy: 0.6614 - 11s/epoch - 38ms/step\n",
      "Epoch 25/100\n",
      "300/300 - 11s - loss: 2.1023 - accuracy: 0.6619 - val_loss: 2.2487 - val_accuracy: 0.6629 - 11s/epoch - 38ms/step\n",
      "Epoch 26/100\n",
      "300/300 - 10s - loss: 2.0847 - accuracy: 0.6641 - val_loss: 2.2707 - val_accuracy: 0.6646 - 10s/epoch - 34ms/step\n",
      "Epoch 27/100\n",
      "300/300 - 10s - loss: 2.0675 - accuracy: 0.6658 - val_loss: 2.2467 - val_accuracy: 0.6655 - 10s/epoch - 34ms/step\n",
      "Epoch 28/100\n",
      "300/300 - 10s - loss: 2.0431 - accuracy: 0.6678 - val_loss: 2.2606 - val_accuracy: 0.6694 - 10s/epoch - 34ms/step\n",
      "Epoch 29/100\n",
      "300/300 - 11s - loss: 2.0228 - accuracy: 0.6700 - val_loss: 2.2546 - val_accuracy: 0.6650 - 11s/epoch - 38ms/step\n",
      "Epoch 30/100\n",
      "300/300 - 10s - loss: 2.0044 - accuracy: 0.6725 - val_loss: 2.2692 - val_accuracy: 0.6678 - 10s/epoch - 34ms/step\n",
      "Epoch 31/100\n",
      "300/300 - 10s - loss: 1.9861 - accuracy: 0.6736 - val_loss: 2.2525 - val_accuracy: 0.6729 - 10s/epoch - 34ms/step\n",
      "Epoch 32/100\n",
      "300/300 - 10s - loss: 1.9686 - accuracy: 0.6759 - val_loss: 2.2275 - val_accuracy: 0.6714 - 10s/epoch - 34ms/step\n",
      "Epoch 33/100\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = load_model('en-de-model.h5')"
   ],
   "metadata": {
    "id": "TJL8JUqJyYw7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_word(n, tokenizer):\n",
    "    if n == 0:\n",
    "        return \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == n:\n",
    "            return word\n",
    "    return \"\""
   ],
   "metadata": {
    "id": "yVwFfW6OycW2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "phrs_enc = encode_sequences(eng_tokenizer, eng_length, [\"the weather is nice today\", \"my name is tom\", \"how old are you\", \"where is the nearest shop\"])\n",
    "print(\"phrs_enc:\", phrs_enc.shape)\n",
    "\n",
    "preds = np.argmax(model.predict(phrs_enc), axis=-1)\n",
    "print(\"Preds:\", preds.shape)\n",
    "print(preds[0])\n",
    "print(get_word(preds[0][0], deu_tokenizer), get_word(preds[0][1], deu_tokenizer), get_word(preds[0][2], deu_tokenizer), get_word(preds[0][3], deu_tokenizer))\n",
    "print(preds[1])\n",
    "print(get_word(preds[1][0], deu_tokenizer), get_word(preds[1][1], deu_tokenizer), get_word(preds[1][2], deu_tokenizer), get_word(preds[1][3], deu_tokenizer))\n",
    "print(preds[2])\n",
    "print(get_word(preds[2][0], deu_tokenizer), get_word(preds[2][1], deu_tokenizer), get_word(preds[2][2], deu_tokenizer), get_word(preds[2][3], deu_tokenizer))\n",
    "print(preds[3])\n",
    "print(get_word(preds[3][0], deu_tokenizer), get_word(preds[3][1], deu_tokenizer), get_word(preds[3][2], deu_tokenizer), get_word(preds[3][3], deu_tokenizer))\n",
    "print()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EN_MCwOjye2F",
    "outputId": "62c3ff39-9c02-4960-87ae-3181ae39bf46",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "phrs_enc: (4, 8)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Preds: (4, 8)\n",
      "[  42    3    3 2736    0    0    0    0]\n",
      "der ist ist vorüber\n",
      "[   2 1354    1  775    0    0    0    0]\n",
      "ich heiße tom name\n",
      "[ 37 166  19   4   0   0   0   0]\n",
      "wie alt sind sie\n",
      "[  77    3   20 2991    0    0    0    0]\n",
      "wo ist die zoo\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "phrs_enc = encode_sequences(eng_tokenizer, eng_length, [\"home\", \"building\", \"house\", \"formation\"])\n",
    "print(\"phrs_enc:\", phrs_enc.shape)\n",
    "\n",
    "preds = np.argmax(model.predict(phrs_enc), axis=-1)\n",
    "print(\"Preds:\", preds.shape)\n",
    "print(preds[0])\n",
    "print(get_word(preds[0][0], deu_tokenizer), get_word(preds[0][1], deu_tokenizer), get_word(preds[0][2], deu_tokenizer), get_word(preds[0][3], deu_tokenizer))\n",
    "print(preds[1])\n",
    "print(get_word(preds[1][0], deu_tokenizer), get_word(preds[1][1], deu_tokenizer), get_word(preds[1][2], deu_tokenizer), get_word(preds[1][3], deu_tokenizer))\n",
    "print(preds[2])\n",
    "print(get_word(preds[2][0], deu_tokenizer), get_word(preds[2][1], deu_tokenizer), get_word(preds[2][2], deu_tokenizer), get_word(preds[2][3], deu_tokenizer))\n",
    "print(preds[3])\n",
    "print(get_word(preds[3][0], deu_tokenizer), get_word(preds[3][1], deu_tokenizer), get_word(preds[3][2], deu_tokenizer), get_word(preds[3][3], deu_tokenizer))\n",
    "print()"
   ],
   "metadata": {
    "id": "1wRMpRpzygjw",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b58186e5-28db-4fd9-cae8-987af218ea50",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "phrs_enc: (4, 8)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Preds: (4, 8)\n",
      "[67 55 67  0  0  0  0  0]\n",
      "hause geht hause \n",
      "[2800    0    0    0    0    0    0    0]\n",
      "bienen   \n",
      "[  1  34 238   0   0   0   0   0]\n",
      "tom aus haus \n",
      "[2800    0    0    0    0    0    0    0]\n",
      "bienen   \n",
      "\n"
     ]
    }
   ]
  }
 ]
}